{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "**Multilayer Perceptron (MLP)**: In this homework you are required to implement and train a 3-layer neural network to classify images of hand-written digits from the MNIST dataset. The input to the network will be a 28 Ã— 28-pixel image, which is converted into a 784-dimensional vector. The output will be a vector of 10 probabilities (one for each digit). Specifically, the network you create should implement a function $g: \\mathbb{R}^{784} \\rightarrow \\mathbb{R}^{10}$, where:\n",
    "\n",
    "$$\\mathbf{z}_{1} = \\mathbf{W}^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)}$$\n",
    "$$\\mathbf{h}_1 = \\mathrm{ReLU}(\\mathbf{z}_1)$$\n",
    "$$\\mathbf{z}_2 = \\mathbf{W}^{(2)}\\mathbf{h}_1 + \\mathbf{b}^{(2)}$$\n",
    "$$\\hat{\\mathbf{y}} = g(\\mathbf{x}) = \\mathrm{Softmax}(\\mathbf{z}_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Propagation**: Compute the intermediate outputs $\\mathbf{z}_{1}$, $\\mathbf{h}_{1}$, $\\mathbf{z}_{2}$, and $\\hat{\\mathbf{y}}$ as the directed graph shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./img/mlp.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function**: After forward propagation, you should use the cross-entropy loss function: \n",
    "$$ f_{\\mathrm{CE}}(\\mathbf{W}^{(1)},\\mathbf{b}^{(1)}, \\mathbf{W}^{(2)}, \\mathbf{b}^{(2)}) =  - \\frac{1}{n}\\sum_{i=1}^{n} \\sum_{k=1}^{10} \\mathbf{y}_k^{(i)} \\log \\hat{\\mathbf{y}}_k^{(i)} $$\n",
    "where $n$ is the number of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backwards Propagation**: To train the neural network, you should use stochastic gradient descent (SGD). \n",
    "\n",
    "# Question 1:\n",
    "Compute the individual gradient for each term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f_{\\mathrm{CE}}}{\\partial \\mathbf{W}^{(2)}} =  [A_{ij}] $$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial f_{\\mathrm{CE}}}{\\partial \\mathbf{b}^{(2)}} =  [B_{i}] $$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial f_{\\mathrm{CE}}}{\\partial \\mathbf{W}^{(1)}} =  [C_{ij}] $$\n",
    "    \n",
    "    \n",
    "$$ \\frac{\\partial f_{\\mathrm{CE}}}{\\partial \\mathbf{b}^{(1)}} =  [D_{i}] $$\n",
    "\n",
    "Where:\n",
    "\n",
    "\\begin{equation}\n",
    "A_{ij} = -\\frac{1}{n}\\sum_{m=1}^{n} \\left\\{\\mathrm{ReLu}(\\mathbf{W}^{(1)}{\\mathbf{x}}^{(m)} + \\mathbf{b}^{(1)})\\right\\}_{j} \\cdot \\left( {{\\mathbf{y}}^{(m)}}^{T} \\cdot [\\mathcal{I}(i) - \\mathbb{1} \\cdot \\hat{y}_{i}^{(m)}]\\right) = \\frac{1}{n}\\sum_{m=1}^{n} \\left\\{\\mathrm{ReLu}(\\mathbf{W}^{(1)}{\\mathbf{x}}^{(m)} + \\mathbf{b}^{(1)})\\right\\}_{j} \\cdot (\\hat{y}_{i}^{(m)} - y_{i}^{(m)})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "B_{i} = -\\frac{1}{n}\\sum_{m=1}^{n} {{\\mathbf{y}}^{(m)}}^{T} \\cdot [\\mathcal{I}(i) - \\mathbb{1} \\cdot \\hat{y}_{i}^{(m)}] = \\frac{1}{n}\\sum_{m=1}^{n} (\\hat{y}_{i}^{(m)} - y_{i}^{(m)})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "C_{ij} = \\frac{1}{n}\\sum_{m=1}^{n} \\left[  \\mathrm{Step}(\\left\\{\\mathbf{W}^{(1)}{\\mathbf{x}}^{(m)} + \\mathbf{b}^{(1)}\\right\\}_{i})\\cdot \\left[\\sum_{k=1}^{10} ({y}_{i}^{(m)} - \\hat y_{i}^{(m)}) \\cdot \\mathbf{W_{ki}}^{(2,m)}  \\right] \\cdot x_j \\right]\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "D_{i} = \\frac{1}{n}\\sum_{m=1}^{n} \\mathrm{Step}(\\left\\{\\mathbf{W}^{(1)}{\\mathbf{x}}^{(m)} + \\mathbf{b}^{(1)}\\right\\}_{i})\\cdot \\left[\\sum_{k=1}^{10} ({y}_{i}^{(m)} - \\hat y_{i}^{(m)}) \\cdot \\mathbf{W_{ki}}^{(2,m)}  \\right]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Notations:\n",
    "\n",
    "$$ n \\text{: the number of examples.}$$\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y}^{(m)} = \\begin{bmatrix}y_{1}^{(m)}\\\\ \\vdots \\\\ y_{10}^{(m)}\\end{bmatrix}\n",
    "\\text{ , real labels of the picture m}\n",
    "\\end{equation}\n",
    "\n",
    "$$ \\mathcal{I}(i): \\text{one-hot vector with only } i^{th} \\text{ element 1} $$\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\hat{y}}^{(m)} = \\frac{1}{\\sum_{k=1}^{10}\\exp{(z_{k,2}})}\\begin{bmatrix}\\exp{(z_{1,2}^{(m)})}\\\\ \\vdots \\\\ \\exp{(z_{10,2}^{(m)})}\\end{bmatrix}\n",
    "\\text{ , predicted labels of the picture m using softmax}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{Step}(x)=\n",
    "\\begin{cases}\n",
    "0 & x<0\\\\\n",
    "1 & x>0\n",
    "\\end{cases}\n",
    "\\text{ , step function}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computational process\n",
    "Notation Unification:\n",
    "![1.jpeg](https://i.loli.net/2019/11/26/GTA6rBjC3YInuma.jpg)\n",
    "\n",
    "#### Calculate Gradient Using Residue, Elemental-wise\n",
    "![2.jpeg](https://i.loli.net/2019/11/26/BeyMUv6FwoCpmAc.jpg)\n",
    "\n",
    "![3.jpeg](https://i.loli.net/2019/11/26/8BeZhSukARcVHX9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: \n",
    "Implement stochastic gradient descent for the network shown above in the *Starter Code* Below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: \n",
    "Verify that your implemented gradient functions are correct using a numerical derivative approximation in *scipy.optimize.check_grad*\n",
    "\n",
    "See the call to check grad in the starter code. \n",
    "\n",
    "Note that: the discrepancy should be less than 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: \n",
    "Train the network using proper hyper-parameters (batch size, learning rate etc), and report the train accuracy and test accuracy in the *Starter Code* Below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE THAT**: You only need to submit this '.ipynb' file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter Code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(trainX):  10000\n",
      "len(testX):  5000\n"
     ]
    }
   ],
   "source": [
    "NUM_INPUT = 784  # Number of input neurons\n",
    "NUM_HIDDEN = 50  # Number of hidden neurons\n",
    "NUM_OUTPUT = 10  # Number of output neurons\n",
    "NUM_CHECK = 5  # Number of examples on which to check the gradient\n",
    "\n",
    "# Given a vector w containing all the weights and biased vectors, extract\n",
    "# and return the individual weights and biases W1, b1, W2, b2.\n",
    "# This is useful for performing a gradient check with check_grad.\n",
    "def unpack (w):  \n",
    "    W1 = np.ndarray(shape=(NUM_HIDDEN,NUM_INPUT))\n",
    "    b1 = np.ndarray(shape=(NUM_HIDDEN,1))\n",
    "    W2 = np.ndarray(shape=(NUM_OUTPUT,NUM_HIDDEN))\n",
    "    b2 = np.ndarray(shape=(NUM_OUTPUT,1))\n",
    "    \n",
    "    [w1, b1, w2, b2] = np.split(w, [W1.size, W1.size + b1.size, W1.size + b1.size + W2.size])\n",
    "    \n",
    "    W1_list = np.split(w1,NUM_HIDDEN)  \n",
    "    for i in range(NUM_HIDDEN):\n",
    "        W1[i] = W1_list[i]\n",
    "        \n",
    "    W2_list = np.split(w2,NUM_OUTPUT)\n",
    "    for i in range(NUM_OUTPUT):\n",
    "        W2[i] = W2_list[i]\n",
    "    \n",
    "    b1 = b1.reshape(len(b1),1)\n",
    "    b2 = b2.reshape(len(b2),1)\n",
    "    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Given individual weights and biases W1, b1, W2, b2, concatenate them and\n",
    "# return a vector w containing all of them.\n",
    "# This is useful for performing a gradient check with check_grad.\n",
    "def pack (W1, b1, W2, b2):\n",
    "    w = W1[:,0]\n",
    "    for i in range(W1.shape[1]-1):\n",
    "        w = np.concatenate([w, W1[:,i+1]], axis = None)\n",
    "    w = np.concatenate([w, b1], axis = None)\n",
    "    for i in range(W2.shape[1]):\n",
    "        w = np.concatenate([w, W2[:,i]], axis = None)\n",
    "    w = np.concatenate([w, b2], axis = None)\n",
    "    \n",
    "    return w\n",
    "\n",
    "# Load the images and labels from a specified dataset (train or test).\n",
    "def loadData (which):\n",
    "    images = np.load(\"data/mnist_{}_images.npy\".format(which))\n",
    "    labels = np.load(\"data/mnist_{}_labels.npy\".format(which))\n",
    "    return images, labels\n",
    "\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the cross-entropy (CE) loss. You might\n",
    "# want to extend this function to return multiple arguments (in which case you\n",
    "# will also need to modify slightly the gradient check code below).\n",
    "\n",
    "def fCE (X, Y, w, batch_size):\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    cost = 0.0\n",
    "    z1_s = np.ndarray(shape=(NUM_HIDDEN,batch_size))\n",
    "    y_pred_s = np.ndarray(shape=(NUM_OUTPUT,batch_size))\n",
    "    \n",
    "    for m in range(batch_size):\n",
    "        z1 = np.dot(W1,X[m].reshape(len(X[m]),1)) + b1\n",
    "        z1_s[:,m] = z1[:,0]    \n",
    "        h1 = np.maximum(0,z1) \n",
    "        z2 = np.dot(W2,h1) + b2 \n",
    "        y_pred = softmax(z2)\n",
    "        y_pred_s[:,m] = y_pred[:,0]\n",
    "        cost -= np.dot(Y[m], np.log(y_pred))\n",
    "        \n",
    "    return z1_s, y_pred_s, cost/batch_size\n",
    "\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the gradient of fCE. You might\n",
    "# want to extend this function to return multiple arguments (in which case you\n",
    "# will also need to modify slightly the gradient check code below).\n",
    "def gradCE (X, Y, w, batch_size):\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    z1_s, y_pred_s, __ = fCE(X, Y, w, batch_size)\n",
    "     \n",
    "    delta_z2_s = np.zeros(shape=(NUM_OUTPUT,batch_size))\n",
    "    delta_z1_s = np.zeros(shape=(NUM_HIDDEN,batch_size))\n",
    "    \n",
    "    grad_W2 = np.zeros(shape=(NUM_OUTPUT,NUM_HIDDEN))\n",
    "    grad_b2 = np.zeros(shape=(NUM_OUTPUT,1))\n",
    "    \n",
    "    grad_W1 = np.zeros(shape=(NUM_HIDDEN,NUM_INPUT))\n",
    "    grad_b1 = np.zeros(shape=(NUM_HIDDEN,1))\n",
    "    \n",
    "    step = lambda x: 1.0 if x > 0.0 else 0.0\n",
    "    \n",
    "    for m in range(batch_size):\n",
    "        for k in range(NUM_OUTPUT):\n",
    "            delta_z2_s[k][m] = y_pred_s[k][m] - Y[m][k]\n",
    "    \n",
    "    for m in range(batch_size):    \n",
    "        for k in range(NUM_HIDDEN):\n",
    "            delta_z1_s[k][m] = step(z1_s[k][m]) * np.dot(delta_z2_s[:,m], W2[:,k])\n",
    "    \n",
    "    for i in range(NUM_OUTPUT):\n",
    "        for j in range(NUM_HIDDEN):\n",
    "            for m in range(batch_size):      \n",
    "                grad_W2[i][j] += delta_z2_s[i][m] * (np.maximum(0,z1_s[:,m]))[j]\n",
    "                grad_b2[i] += delta_z2_s[i][m]\n",
    "            grad_W2[i][j] /= batch_size\n",
    "            grad_b2[i] /= batch_size\n",
    "    \n",
    "    for i in range(NUM_HIDDEN):\n",
    "        for j in range(NUM_INPUT):\n",
    "            for m in range(batch_size):\n",
    "                grad_W1[i][j] += delta_z1_s[i][m] * X[m][j]\n",
    "                grad_b1[i] += delta_z1_s[i][m]\n",
    "            grad_W1[i][j] /= batch_size\n",
    "            grad_b1[i] /= batch_size\n",
    "    \n",
    "    grad = pack(grad_W1.T, grad_b1, grad_W2.T, grad_b2)\n",
    "  \n",
    "    return grad\n",
    "\n",
    "# Given training and testing datasets and an initial set of weights/biases b,\n",
    "# train the NN.\n",
    "## return the train accuracy and the test accuracy\n",
    "def train (trainX, trainY, testX, testY, w):\n",
    "    # hyper parameters\n",
    "    lrn_rate = 0.01\n",
    "    batch_size = 50\n",
    "    error = 0.01\n",
    "    \n",
    "    i = 0\n",
    "    train_acc = 0\n",
    "    test_acc = 0\n",
    "    w_new = w\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    \n",
    "    while(True):         \n",
    "        train_X_batch = trainX[i:i + batch_size,:]\n",
    "        train_Y_batch = trainY[i:i + batch_size,:]\n",
    "        \n",
    "        i += batch_size\n",
    "        if (i >= len(trainX)):#shuffle data after one epoch\n",
    "            i = 0\n",
    "            train = np.concatenate([trainX, trainY], axis = 1)\n",
    "            np.random.shuffle(train)\n",
    "            trainX, trainY = np.split(train, [trainX.shape[1]], axis = 1)\n",
    "        \n",
    "        grad = gradCE (train_X_batch, train_Y_batch, w_new, batch_size)\n",
    "        norm = np.linalg.norm(grad)\n",
    "        \n",
    "        if (norm < error):#use 2-norm of the gradient to decide when to end training\n",
    "            __, y_pred_s_test, __ = fCE(trainX, trainY, w_new, len(trainX))\n",
    "            for j in range(len(trainX)):\n",
    "                index = np.argmax(y_pred_s_train[:,j])\n",
    "                if (trainY[index] == 1):\n",
    "                    train_acc += 1\n",
    "            train_acc = train_acc/len(trainX)#calculating training accuracy\n",
    "            break\n",
    "        \n",
    "        grad_W1, grad_b1, grad_W2, grad_b2 = unpack(grad)  \n",
    "        W1 -= lrn_rate * grad_W1\n",
    "        b1 -= lrn_rate * grad_b1\n",
    "        W2 -= lrn_rate * grad_W2\n",
    "        b2 -= lrn_rate * grad_b2\n",
    "        w_new = pack(W1.T, b1, W2.T, b2)\n",
    "    \n",
    "    __, y_pred_s_test, __ = fCE(testX, testY, w_new, len(testX))\n",
    "    for j in range(len(testX)):\n",
    "        index = np.argmax(y_pred_s_test[:,j])\n",
    "        if (testY[index] == 1):\n",
    "            test_acc += 1\n",
    "    test_acc = test_acc/len(testX)#calculating test accuracy\n",
    "    \n",
    "    return train_acc, test_acc\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    trainX, trainY = loadData(\"train\")\n",
    "    testX, testY = loadData(\"test\")\n",
    "\n",
    "    print(\"len(trainX): \", len(trainX))\n",
    "    print(\"len(testX): \", len(testX))\n",
    "\n",
    "    # Initialize weights randomly\n",
    "    W1 = 2*(np.random.random(size=(NUM_INPUT, NUM_HIDDEN))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
    "    b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
    "    W2 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_OUTPUT))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "    b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
    "    w = pack(W1, b1.reshape(len(b1),1), W2, b2.reshape(len(b2),1))\n",
    "    \n",
    "    # Check that the gradient is correct on just a few examples (randomly drawn).\n",
    "    idxs = np.random.permutation(trainX.shape[0])[0:NUM_CHECK]\n",
    "\n",
    "    discrepancy = scipy.optimize.check_grad(lambda w_: fCE(np.atleast_2d(trainX[idxs,:]), np.atleast_2d(trainY[idxs,:]), w_, NUM_CHECK)[2], lambda w_: gradCE(np.atleast_2d(trainX[idxs,:]), np.atleast_2d(trainY[idxs,:]), w_, NUM_CHECK), w)\n",
    "    if discrepancy < 0.01:\n",
    "        print(\"My implemented cost and gradient functions are correct\")\n",
    "\n",
    "    # Train the network and return the train accuracy and test accuracy\n",
    "#     train_acc, test_acc = train(trainX, trainY, testX, testY, w)\n",
    "#     print(train_acc,test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121.3574398550274"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrepancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Due to unknown reasons, the discrepency does not satisfy the demand of less than 0.01 (usually fluctuate around 0.1). This is possibly due to minor mistakes in the code or some procedures' numerical accuracy does not satisfy our needs (quite possibly since different methods to average gradient by batch size that are logically same (elemental-wise or matrix-wise) can result in significant difference). Whatever the reason, the gradient seems not converging and this results in the failure of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_ass08]",
   "language": "python",
   "name": "conda-env-env_ass08-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
