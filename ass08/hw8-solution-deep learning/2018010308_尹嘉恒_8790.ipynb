{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "**Multilayer Perceptron (MLP)**: In this homework you are required to implement and train a 3-layer neural network to classify images of hand-written digits from the MNIST dataset. The input to the network will be a 28 × 28-pixel image, which is converted into a 784-dimensional vector. The output will be a vector of 10 probabilities (one for each digit). Specifically, the network you create should implement a function $g: \\mathbb{R}^{784} \\rightarrow \\mathbb{R}^{10}$, where:\n",
    "\n",
    "$$\\mathbf{z}_{1} = \\mathbf{W}^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)}$$\n",
    "$$\\mathbf{h}_1 = ReLU(\\mathbf{z}_1)$$\n",
    "$$\\mathbf{z}_2 = \\mathbf{W}^{(2)}\\mathbf{h}_1 + \\mathbf{b}^{(2)}$$\n",
    "$$\\hat{\\mathbf{y}} = g(\\mathbf{x}) = Softmax(\\mathbf{z}_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Propagation**: Compute the intermediate outputs $\\mathbf{z}_{1}$, $\\mathbf{h}_{1}$, $\\mathbf{z}_{2}$, and $\\hat{\\mathbf{y}}$ as the directed graph shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./img/mlp.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function**: After forward propagation, you should use the cross-entropy loss function: \n",
    "$$ f_{CE}(\\mathbf{W}^{(1)},\\mathbf{b}^{(1)}, \\mathbf{W}^{(2)}, \\mathbf{b}^{(2)}) =  - \\frac{1}{n}\\sum_{i=1}^{n} \\sum_{k=1}^{10} \\mathbf{y}_k^{(i)} \\log \\hat{\\mathbf{y}}_k^{(i)} $$\n",
    "where $n$ is the number of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backwards Propagation**: To train the neural network, you should use stochastic gradient descent (SGD). \n",
    "\n",
    "# Question 1:\n",
    "Compute the individual gradient for each term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{W}^{(2)}} =    - \\frac{1}{n}\\sum_{i=1}^{n} (\\mathbf{y}^{(i)} - \\hat{\\mathbf{y}}^{(i)}) \\mathbf{h}_1^T$$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{b}^{(2)}} =    - \\frac{1}{n}\\sum_{i=1}^{n} (\\mathbf{y}^{(i)} - \\hat{\\mathbf{y}}^{(i)}) $$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{W}^{(1)}} =    - \\frac{1}{n}\\sum_{i=1}^{n} \\frac{d \\mathbf{h}_1}{d \\mathbf{z}_1} {\\mathbf{W}^{(2)}}^T  (\\mathbf{y}^{(i)} - \\hat{\\mathbf{y}}^{(i)})  \\mathbf{x}^T $$\n",
    "    \n",
    "    \n",
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{b}^{(1)}} =     - \\frac{1}{n}\\sum_{i=1}^{n} \\frac{d \\mathbf{h}_1}{d \\mathbf{z}_1} {\\mathbf{W}^{(2)}}^T  (\\mathbf{y}^{(i)} - \\hat{\\mathbf{y}}^{(i)})  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: \n",
    "Implement stochastic gradient descent for the network shown above in the *Starter Code* Below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: \n",
    "Verify that your implemented gradient functions are correct using a numerical derivative approximation in *scipy.optimize.check_grad*\n",
    "\n",
    "See the call to check grad in the starter code. \n",
    "\n",
    "Note that: the discrepancy should be less than 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: \n",
    "Train the network using proper hyper-parameters (batch size, learning rate etc), and report the train accuracy and test accuracy in the *Starter Code* Below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE THAT**: You only need to submit this '.ipynb' file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter Code: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W1, b1, W2, b2的封装与拆解："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a vector w containing all the weights and biased vectors, extract\n",
    "# and return the individual weights and biases W1, b1, W2, b2.\n",
    "# This is useful for performing a gradient check with check_grad.\n",
    "def unpack (w):\n",
    "    W1 = w[0:NUM_INPUT*NUM_HIDDEN].reshape(NUM_INPUT,NUM_HIDDEN)\n",
    "    b1 = w[NUM_INPUT*NUM_HIDDEN:NUM_INPUT*NUM_HIDDEN+NUM_HIDDEN].reshape(NUM_HIDDEN)\n",
    "    W2 = w[NUM_INPUT*NUM_HIDDEN+NUM_HIDDEN:NUM_INPUT*NUM_HIDDEN+NUM_HIDDEN+NUM_HIDDEN*NUM_OUTPUT].reshape(NUM_HIDDEN,NUM_OUTPUT)\n",
    "    b2 = w[-NUM_OUTPUT:].reshape(NUM_OUTPUT)\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Given individual weights and biases W1, b1, W2, b2, concatenate them and\n",
    "# return a vector w containing all of them.\n",
    "# This is useful for performing a gradient check with check_grad.\n",
    "# def pack (W1, b1, W2, b2):\n",
    "\n",
    "def pack (W1, b1, W2, b2):\n",
    "    W1f = W1.reshape(NUM_INPUT*NUM_HIDDEN)\n",
    "    W2f = W2.reshape(NUM_HIDDEN*NUM_OUTPUT)\n",
    "    b1f = b1.reshape(NUM_HIDDEN)\n",
    "    b2f = b2.reshape(NUM_OUTPUT)\n",
    "    w = np.concatenate((W1f,b1f,W2f,b2f),axis=0)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义ReLU、softmax和ReLU的梯度函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    x1 = x\n",
    "    for i in range(NUM_HIDDEN):\n",
    "        if(x[i]<0):\n",
    "            x1[i]=0.\n",
    "    return x1\n",
    "\n",
    "def softmax(x):\n",
    "    x = np.exp(x)/sum(np.exp(x))\n",
    "    return x\n",
    "\n",
    "def d_ReLU(h1):\n",
    "    d = np.zeros((NUM_HIDDEN,NUM_HIDDEN))\n",
    "    for i in range(NUM_HIDDEN):\n",
    "        if(h1[i]):\n",
    "            d[i][i]=1. \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fCE："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the cross-entropy (CE) loss. You might\n",
    "# want to extend this function to return multiple arguments (in which case you\n",
    "# will also need to modify slightly the gradient check code below).\n",
    "def fCE (X, Y, w):\n",
    "    cost = 0\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    n = len(X)\n",
    "    for i in range(n):\n",
    "        z1 = np.dot(X[i],W1)+b1\n",
    "        h1 = ReLU(z1)\n",
    "        z2 = np.dot(h1,W2)+b2\n",
    "        y = softmax(z2)\n",
    "        cost += (-1/n)*np.dot(Y[i],np.log(y).transpose())\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradCE："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the gradient of fCE. You might\n",
    "# want to extend this function to return multiple arguments (in which case you\n",
    "# will also need to modify slightly the gradient check code below).\n",
    "def gradCE(X,Y,w):\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    # initialize\n",
    "    grad_W1 = W1*0.\n",
    "    grad_b1 = b1*0.\n",
    "    grad_W2 = W2*0.\n",
    "    grad_b2 = b2*0.\n",
    "    n = len(X)\n",
    "    # compute the grad\n",
    "    for i in range(n):\n",
    "        z1 = np.dot(X[i],W1)+b1\n",
    "        h1 = ReLU(z1)\n",
    "        z2 = np.dot(h1,W2)+b2\n",
    "        y = softmax(z2)\n",
    "        # single time compute        \n",
    "        grad_b2_s = Y[i]-y\n",
    "        grad_W2_s = np.dot(h1.transpose().reshape(-1,1),grad_b2_s.reshape(1,-1))\n",
    "        grad_b1_s = np.dot(np.dot(grad_b2_s,W2.transpose()),d_ReLU(h1))\n",
    "        grad_W1_s = (np.dot(X[i].transpose().reshape(NUM_INPUT,1),np.dot(grad_b2_s,np.dot(W2.transpose(),d_ReLU(h1))).reshape(1,NUM_HIDDEN)))\n",
    "        # sum and divided by n\n",
    "        grad_b2 += (-1./n)*grad_b2_s\n",
    "        grad_W2 += (-1./n)*grad_W2_s\n",
    "        grad_b1 += (-1./n)*grad_b1_s\n",
    "        grad_W1 += (-1./n)*grad_W1_s\n",
    "    # pack\n",
    "    w_grad = pack(grad_W1,grad_b1,grad_W2,grad_b2)\n",
    "    return w_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given training and testing datasets and an initial set of weights/biases b,\n",
    "# train the NN.\n",
    "## return the train accuracy and the test accuracy\n",
    "def compute(X,w):\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    z1 = np.dot(X,W1)+b1\n",
    "    h1 = ReLU(z1)\n",
    "    z2 = np.dot(h1,W2)+b2\n",
    "    y_pred = softmax(z2)\n",
    "    return y_pred\n",
    "\n",
    "def train (trainX, trainY, testX, testY, w, BATCH_SIZE = 64,NUM_ITERATION = 150):\n",
    "    rate = 0.05\n",
    "    test_acc = 0\n",
    "    for i in range(NUM_ITERATION):\n",
    "#         print(\"NUM_ITERATION:\",i)\n",
    "        idxs = np.random.permutation(trainX.shape[0])[0:BATCH_SIZE]\n",
    "        X = np.atleast_2d(trainX[idxs,:])\n",
    "        Y = np.atleast_2d(trainY[idxs,:])\n",
    "        train_acc_num = 0\n",
    "        for j in range(BATCH_SIZE):\n",
    "            y_pred = compute(X[j],w)\n",
    "            index = np.argwhere(Y[j]==1)[0][0]\n",
    "            index_pred = np.argwhere(y_pred==np.max(y_pred))[0][0]\n",
    "            if(index_pred == index):\n",
    "                train_acc_num +=1 \n",
    "            w = w-rate*gradCE(X,Y,w)\n",
    "        train_acc = train_acc_num/BATCH_SIZE\n",
    "        if train_acc>0.9:\n",
    "            rate = 0.0001\n",
    "        elif (train_acc>0.8) and (train_acc<=0.9):\n",
    "            rate = 0.005\n",
    "        else:\n",
    "            rate = 0.05\n",
    "    \n",
    "    for j in range(len(testX)):\n",
    "        y_pred = compute(testX[j],w)\n",
    "        index = np.argwhere(testY[j]==1)[0][0]\n",
    "        index_pred = np.argwhere(y_pred==np.max(y_pred))[0][0]\n",
    "        if(index_pred == index):\n",
    "            test_acc +=1\n",
    "    test_acc = test_acc/len(testX)\n",
    "\n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(trainX):  10000\n",
      "len(testX):  5000\n",
      "1.279702296059427e-06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize\n",
    "\n",
    "NUM_INPUT = 784  # Number of input neurons\n",
    "NUM_HIDDEN = 50  # Number of hidden neurons\n",
    "NUM_OUTPUT = 10  # Number of output neurons\n",
    "NUM_CHECK = 5  # Number of examples on which to check the gradient\n",
    "\n",
    "# Load the images and labels from a specified dataset (train or test).\n",
    "def loadData (which):\n",
    "    images = np.load(\"data/mnist_{}_images.npy\".format(which))\n",
    "    labels = np.load(\"data/mnist_{}_labels.npy\".format(which))\n",
    "    return images, labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    trainX, trainY = loadData(\"train\")\n",
    "    testX, testY = loadData(\"test\")\n",
    "    print(\"len(trainX): \", len(trainX))\n",
    "    print(\"len(testX): \", len(testX))\n",
    "    \n",
    "    # Initialize weights randomly\n",
    "    W1 = 2*(np.random.random(size=(NUM_INPUT, NUM_HIDDEN))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
    "    b1 = 0.01 * np.ones((1,NUM_HIDDEN))\n",
    "    W2 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_OUTPUT))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "    b2 = 0.01 * np.ones((1,NUM_OUTPUT))\n",
    "    w = pack(W1, b1, W2, b2)\n",
    "\n",
    "    # Check that the gradient is correct on just a few examples (randomly drawn).\n",
    "    idxs = np.random.permutation(trainX.shape[0])[0:NUM_CHECK]\n",
    "    discrepancy = scipy.optimize.check_grad(lambda w_: fCE(np.atleast_2d(trainX[idxs,:]), np.atleast_2d(trainY[idxs,:]), w_), \\\n",
    "                                    lambda w_: gradCE(np.atleast_2d(trainX[idxs,:]), np.atleast_2d(trainY[idxs,:]), w_), \\\n",
    "                                    w)\n",
    "    print(\"discrepancy\",discrepancy)\n",
    "    if discrepancy < 0.01:\n",
    "        print(\"My implemented cost and gradient functions are correct\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.984375 0.8934\n"
     ]
    }
   ],
   "source": [
    "# Train the network and return the train accuracy and test accuracy\n",
    "train_acc, test_acc=train(trainX, trainY, testX, testY, w, BATCH_SIZE = 64,NUM_ITERATION = 150)\n",
    "print(\"train_acc:{}\\ntest_acc:{}\".format(train_acc,test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
