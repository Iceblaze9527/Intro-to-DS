{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "**Multilayer Perceptron (MLP)**: In this homework you are required to implement and train a 3-layer neural network to classify images of hand-written digits from the MNIST dataset. The input to the network will be a 28 × 28-pixel image, which is converted into a 784-dimensional vector. The output will be a vector of 10 probabilities (one for each digit). Specifically, the network you create should implement a function $g: \\mathbb{R}^{784} \\rightarrow \\mathbb{R}^{10}$, where:\n",
    "\n",
    "$$\\mathbf{z}_{1} = \\mathbf{W}^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)}$$\n",
    "$$\\mathbf{h}_1 = ReLU(\\mathbf{z}_1)$$\n",
    "$$\\mathbf{z}_2 = \\mathbf{W}^{(2)}\\mathbf{h}_1 + \\mathbf{b}^{(2)}$$\n",
    "$$\\hat{\\mathbf{y}} = g(\\mathbf{x}) = Softmax(\\mathbf{z}_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Propagation**: Compute the intermediate outputs $\\mathbf{z}_{1}$, $\\mathbf{h}_{1}$, $\\mathbf{z}_{2}$, and $\\hat{\\mathbf{y}}$ as the directed graph shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./img/mlp.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function**: After forward propagation, you should use the cross-entropy loss function: \n",
    "$$ f_{CE}(\\mathbf{W}^{(1)},\\mathbf{b}^{(1)}, \\mathbf{W}^{(2)}, \\mathbf{b}^{(2)}) =  - \\frac{1}{n}\\sum_{i=1}^{n} \\sum_{k=1}^{10} \\mathbf{y}_k^{(i)} \\log \\hat{\\mathbf{y}}_k^{(i)} $$\n",
    "where $n$ is the number of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backwards Propagation**: To train the neural network, you should use stochastic gradient descent (SGD). \n",
    "\n",
    "# Question 1:\n",
    "Compute the individual gradient for each term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{W}^{(2)}}  =  \\frac{1}{n}\\sum_{i=1}^{n}  (\\hat{\\mathbf{y}}^{(i)} - \\mathbf{y}^{(i)})  (\\mathbf{h_1}^{(i)})^{T}  $$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{b}^{(2)}} =  \\frac{1}{n}\\sum_{i=1}^{n}  (\\hat{\\mathbf{y}}^{(i)} - \\mathbf{y}^{(i)})   $$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{W}^{(1)}} = \\frac{1}{n}\\sum_{i=1}^{n}  ((\\hat{\\mathbf{y}}^{(i)} - \\mathbf{y}^{(i)})^{T} \\mathbf W^{(2)})^{T} \\circ sgn(\\mathbf{z_1}^{(i)}) (\\mathbf{x}^{(i)})^{T}$$ \n",
    "    \n",
    "    \n",
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{b}^{(1)}} =  \\frac{1}{n}\\sum_{i=1}^{n}  ((\\hat{\\mathbf{y}}^{(i)} - \\mathbf{y}^{(i)})^{T} \\mathbf W^{(2)})^{T} \\circ sgn(\\mathbf{z_1}^{(i)})   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: \n",
    "Implement stochastic gradient descent for the network shown above in the *Starter Code* Below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: \n",
    "Verify that your implemented gradient functions are correct using a numerical derivative approximation in *scipy.optimize.check_grad*\n",
    "\n",
    "See the call to check grad in the starter code. \n",
    "\n",
    "Note that: the discrepancy should be less than 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: \n",
    "Train the network using proper hyper-parameters (batch size, learning rate etc), and report the train accuracy and test accuracy in the *Starter Code* Below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE THAT**: You only need to submit this '.ipynb' file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter Code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import scipy.special\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_INPUT = 784  # Number of input neurons\n",
    "NUM_HIDDEN = 50  # Number of hidden neurons\n",
    "NUM_OUTPUT = 10  # Number of output neurons\n",
    "NUM_CHECK = 5  # Number of examples on which to check the gradient\n",
    "\n",
    "def relu(X):\n",
    "    return (np.abs(X) + X)/2\n",
    "\n",
    "def relu_back(X):\n",
    "    X_grad = np.copy(X)\n",
    "    X_grad[X_grad <= 0] = 0\n",
    "    X_grad[X_grad > 0] = 1\n",
    "    return X_grad\n",
    "\n",
    "def softmax(X):\n",
    "    t = np.exp(X)\n",
    "    a = np.exp(X) / np.sum(t, axis=0, keepdims=True) # compute softamx by column\n",
    "    return a\n",
    "\n",
    "def cross_entropy(Y_hat, Y):\n",
    "    N = Y_hat.shape[1]\n",
    "    ce = -np.sum(Y*np.log(Y_hat))/N\n",
    "    return ce\n",
    "\n",
    "# data = np.array([[5,4,3],[5,4,3]])\n",
    "# print(softmax(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a vector w containing all the weights and biased vectors, extract\n",
    "# and return the individual weights and biases W1, b1, W2, b2.\n",
    "# This is useful for performing a gradient check with check_grad.\n",
    "def unpack (w):\n",
    "    # W1 of (50, 784), b1 of (50, 1), W2 of (10, 50), b2 of (10, 1)\n",
    "    W1 = w[ : NUM_HIDDEN*NUM_INPUT].reshape(NUM_HIDDEN, NUM_INPUT)\n",
    "    b1 = w[NUM_HIDDEN*NUM_INPUT : NUM_HIDDEN*NUM_INPUT + NUM_HIDDEN].reshape(NUM_HIDDEN, 1)\n",
    "    W2 = w[NUM_HIDDEN*NUM_INPUT+NUM_HIDDEN : NUM_HIDDEN*NUM_INPUT+NUM_HIDDEN+NUM_OUTPUT*NUM_HIDDEN].reshape(NUM_OUTPUT, NUM_HIDDEN)\n",
    "    b2 = w[NUM_HIDDEN*NUM_INPUT+NUM_HIDDEN+NUM_OUTPUT*NUM_HIDDEN : ].reshape(NUM_OUTPUT, 1)\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Given individual weights and biases W1, b1, W2, b2, concatenate them and\n",
    "# return a vector w containing all of them.\n",
    "# This is useful for performing a gradient check with check_grad.\n",
    "def pack (W1, b1, W2, b2):\n",
    "    w = np.concatenate((W1.flatten(), b1.flatten(), W2.flatten(), b2.flatten()))\n",
    "    return w\n",
    "\n",
    "# Load the images and labels from a specified dataset (train or test).\n",
    "def loadData (which):\n",
    "    images = np.load(\"data/mnist_{}_images.npy\".format(which))\n",
    "    labels = np.load(\"data/mnist_{}_labels.npy\".format(which))\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the cross-entropy (CE) loss. You might\n",
    "# want to extend this function to return multiple arguments (in which case you\n",
    "# will also need to modify slightly the gradient check code below).\n",
    "def fCE (X, Y, w):\n",
    "    # X.shape= (784, 10000), Y.shape= (10, 10000)\n",
    "    # W1 of (50, 784), b1 of (50, 1), W2 of (10, 50), b2 of (10, 1)\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    Z1 = np.matmul(W1, X) + b1\n",
    "    H1 = relu(Z1)\n",
    "    # H1.shape= (50, 10000)\n",
    "    Z2 = np.matmul(W2, H1) + b2\n",
    "    Y_hat = softmax(Z2)\n",
    "    # Y_hat.shape(10, 10000)\n",
    "    cost = cross_entropy(Y_hat, Y)\n",
    "    return cost, Y_hat\n",
    "\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the gradient of fCE. You might\n",
    "# want to extend this function to return multiple arguments (in which case you\n",
    "# will also need to modify slightly the gradient check code below).\n",
    "def gradCE (X, Y, w):\n",
    "    # X.shape= (784, 10000), Y.shape= (10, 10000)\n",
    "    # W1 of (50, 784), b1 of (50, 1), W2 of (10, 50), b2 of (10, 1)\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    \n",
    "    Z1 = np.matmul(W1, X) + b1\n",
    "    H1 = relu(Z1)\n",
    "    Z2 = np.matmul(W2, H1) + b2\n",
    "    Y_hat = softmax(Z2)\n",
    "    \n",
    "    grad_b2 = np.mean(Y_hat - Y, axis=1).reshape(-1, 1)\n",
    "    # grad_b2 of (10, 1)\n",
    "    grad_W2 = np.matmul(Y_hat-Y, H1.T) / X.shape[1]\n",
    "    # grad_W2 of (10, 50)\n",
    "    \n",
    "    delta = np.matmul((Y_hat - Y).T, W2) * relu_back(Z1.T)\n",
    "    # shape of np.matmul((Y_hat - Y).T, W2) is (10000, 50), delta of (10000, 50)\n",
    "    grad_b1 = np.mean(delta.T, axis=1).reshape(-1, 1)\n",
    "    # grad_b1 of (50, 1)\n",
    "    grad_W1 = np.matmul(delta.T, X.T) / X.shape[1]\n",
    "    # grad_W1 of (50, 784)\n",
    "    \n",
    "    grad = pack(grad_W1, grad_b1, grad_W2, grad_b2)\n",
    "    return grad\n",
    "\n",
    "# Given training and testing datasets and an initial set of weights/biases b,\n",
    "# train the NN.\n",
    "## return the train accuracy and the test accuracy\n",
    "def train (trainX, trainY, testX, testY, w):\n",
    "    train_acc, test_acc = [], []\n",
    "\n",
    "    epochs = 500\n",
    "    minibatch = 32\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # print(\"Training epoch {}\".format(epoch))\n",
    "        for batch in range(int(trainX.shape[1]/minibatch)):\n",
    "            Xbatch = trainX[:, batch*minibatch:(batch+1)*minibatch]\n",
    "            Ybatch = trainY[:, batch*minibatch:(batch+1)*minibatch]\n",
    "            w_grad = gradCE(Xbatch, Ybatch, w)\n",
    "            w = w - w_grad * learning_rate\n",
    "        \n",
    "        CE, Y_hat = fCE(trainX, trainY, w)\n",
    "        pred = np.argmax(Y_hat, axis=0)\n",
    "        gold = np.argmax(trainY, axis=0)\n",
    "        train_acc.append(np.mean(pred == gold))\n",
    "        # print(\"train_acc:\", np.mean(pred == gold))\n",
    "        \n",
    "        CE, Y_hat = fCE(testX, testY, w)\n",
    "        pred = np.argmax(Y_hat, axis=0)\n",
    "        gold = np.argmax(testY, axis=0)\n",
    "        # print(\"test_acc:\", np.mean(pred == gold))\n",
    "        test_acc.append(np.mean(pred == gold))\n",
    "\n",
    "    return np.array(train_acc), np.array(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(trainX):  10000\n",
      "len(testX):  5000\n",
      "My implemented cost and gradient functions are correct\n",
      "total epochs trained:  500\n",
      "best accuracy 0.9442 is reached at epoch 365\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    trainX, trainY = loadData(\"train\")\n",
    "    testX, testY = loadData(\"test\")\n",
    "\n",
    "    print(\"len(trainX): \", len(trainX))\n",
    "    print(\"len(testX): \", len(testX))\n",
    "    \n",
    "    # Initialize weights randomly\n",
    "    W1 = 2*(np.random.random(size=(NUM_INPUT, NUM_HIDDEN))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
    "    b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
    "    W2 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_OUTPUT))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "    b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
    "    \n",
    "    # 根据顶部的表达式，各参数的尺寸应该为(对于单样本)：\n",
    "    # x of (784, 1)\n",
    "    # W1 of (50, 784), b1 of (50, 1), W2 of (10, 50), b2 of (10, 1)\n",
    "    # 作业中所给出的数据格式不是很符合惯例，进行转置调整\n",
    "    \n",
    "    trainX, trainY = trainX.T, trainY.T\n",
    "    testX, testY = testX.T, testY.T\n",
    "    W1, W2 = W1.T, W2.T\n",
    "    \n",
    "    # print(trainX.shape, testX.shape)\n",
    "    # print(W1.shape, b1.shape, W2.shape, b2.shape)\n",
    "    # 调整之后身心舒畅，可以按照第一题中的表达式进行计算梯度\n",
    "    \n",
    "    w = pack(W1, b1, W2, b2)\n",
    "    \n",
    "    # Check that the gradient is correct on just a few examples (randomly drawn).\n",
    "    idxs = np.random.permutation(trainX.shape[0])[0:NUM_CHECK]\n",
    "    discrepancy = scipy.optimize.check_grad(lambda w_: fCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[:,idxs]), w_)[0], \\\n",
    "                                    lambda w_: gradCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[:,idxs]), w_), \\\n",
    "                                    w)\n",
    "    \n",
    "    if discrepancy < 0.01:\n",
    "        print(\"My implemented cost and gradient functions are correct\")\n",
    "\n",
    "    # Train the network and return the train accuracy and test accuracy\n",
    "    train_acc, test_acc = train(trainX, trainY, testX, testY, w)\n",
    "    epochs = len(train_acc)\n",
    "    max_iter = test_acc.argmax()\n",
    "    max_acc = test_acc.max()\n",
    "    \n",
    "    print(\"total epochs trained: \", epochs)\n",
    "    print(\"best accuracy {} is reached at epoch {}\".format(max_acc, max_iter))\n",
    "    \n",
    "    plt.plot(range(epochs), train_acc)\n",
    "    plt.plot(range(epochs), test_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
