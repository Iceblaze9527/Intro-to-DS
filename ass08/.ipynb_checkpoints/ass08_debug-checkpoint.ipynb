{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(trainX):  10000\n",
      "len(testX):  5000\n"
     ]
    }
   ],
   "source": [
    "NUM_INPUT = 784  # Number of input neurons\n",
    "NUM_HIDDEN = 50  # Number of hidden neurons\n",
    "NUM_OUTPUT = 10  # Number of output neurons\n",
    "NUM_CHECK = 5  # Number of examples on which to check the gradient\n",
    "\n",
    "def unpack (w):  \n",
    "    W1 = np.ndarray(shape=(NUM_HIDDEN,NUM_INPUT))\n",
    "    b1 = np.ndarray(shape=(NUM_HIDDEN,1))\n",
    "    W2 = np.ndarray(shape=(NUM_OUTPUT,NUM_HIDDEN))\n",
    "    b2 = np.ndarray(shape=(NUM_OUTPUT,1))\n",
    "    \n",
    "    [w1, b1_f, w2, b2_f] = np.split(w, [W1.size, W1.size + b1.size, W1.size + b1.size + W2.size])\n",
    "    \n",
    "    W1_list = np.split(w1,NUM_HIDDEN)  \n",
    "    for i in range(NUM_HIDDEN):\n",
    "        W1[i] = W1_list[i]\n",
    "        \n",
    "    W2_list = np.split(w2,NUM_OUTPUT)\n",
    "    for i in range(NUM_OUTPUT):\n",
    "        W2[i] = W2_list[i]\n",
    "    \n",
    "    b1 = b1_f.reshape(len(b1_f),1)\n",
    "    b2 = b2_f.reshape(len(b2_f),1)\n",
    "    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def pack (W1, b1, W2, b2):\n",
    "    w = W1[:,0]\n",
    "    for i in range(W1.shape[1]-1):\n",
    "        w = np.concatenate([w, W1[:,i+1]], axis = None)\n",
    "    w = np.concatenate([w, b1], axis = None)\n",
    "    for i in range(W2.shape[1]):\n",
    "        w = np.concatenate([w, W2[:,i]], axis = None)\n",
    "    w = np.concatenate([w, b2], axis = None)\n",
    "    \n",
    "    return w\n",
    "\n",
    "def loadData (which):\n",
    "    images = np.load(\"data/mnist_{}_images.npy\".format(which))\n",
    "    labels = np.load(\"data/mnist_{}_labels.npy\".format(which))\n",
    "    return images, labels\n",
    "\n",
    "def fCE (X, Y, w, batch_size):\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    cost = 0.0\n",
    "    z1_s = np.zeros(shape=(NUM_HIDDEN,batch_size))\n",
    "    y_pred_s = np.zeros(shape=(NUM_OUTPUT,batch_size))\n",
    "    \n",
    "    for m in range(batch_size):\n",
    "        z1 = np.dot(W1,X[m].reshape(len(X[m]),1)) + b1 \n",
    "        z1_s[:,m] = z1[:,0]    \n",
    "        h1 = np.maximum(0,z1) \n",
    "        z2 = np.dot(W2,h1) + b2 \n",
    "        y_pred = softmax(z2)\n",
    "        y_pred_s[:,m] = y_pred[:,0]\n",
    "        \n",
    "        cost -= np.dot(Y[m], np.log(y_pred))\n",
    "    \n",
    "    cost /= batch_size\n",
    "        \n",
    "    return z1_s, y_pred_s, cost\n",
    "\n",
    "def gradCE (X, Y, w, batch_size):\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    z1_s, y_pred_s, __ = fCE(X, Y, w, batch_size)\n",
    "     \n",
    "    delta_z2_s = np.zeros(shape=(NUM_OUTPUT,batch_size))\n",
    "    delta_z1_s = np.zeros(shape=(NUM_HIDDEN,batch_size))\n",
    "    \n",
    "    grad_W2 = np.zeros(shape=(NUM_OUTPUT,NUM_HIDDEN))\n",
    "    grad_b2 = np.zeros(shape=(NUM_OUTPUT,1))\n",
    "    \n",
    "    grad_W1 = np.zeros(shape=(NUM_HIDDEN,NUM_INPUT))\n",
    "    grad_b1 = np.zeros(shape=(NUM_HIDDEN,1))\n",
    "    \n",
    "    step = lambda x: 1.0 if x > 1.0e-32 else 0.0\n",
    "    \n",
    "    for m in range(batch_size):\n",
    "        for k in range(NUM_OUTPUT):\n",
    "            delta_z2_s[k][m] = y_pred_s[k][m] - Y[m][k]\n",
    "    \n",
    "    for m in range(batch_size):    \n",
    "        for k in range(NUM_HIDDEN):\n",
    "            delta_z1_s[k][m] = step(z1_s[k][m]) * np.dot(delta_z2_s[:,m], W2[:,k])\n",
    "    \n",
    "    for i in range(NUM_OUTPUT):\n",
    "        for j in range(NUM_HIDDEN):\n",
    "            for m in range(batch_size):      \n",
    "                grad_W2[i][j] += delta_z2_s[i][m] * (np.maximum(0,z1_s[:,m]))[j]\n",
    "                grad_b2[i] += delta_z2_s[i][m]\n",
    "            grad_W2[i][j] /= batch_size\n",
    "            grad_b2[i] /= batch_size\n",
    "    \n",
    "    for i in range(NUM_HIDDEN):\n",
    "        for j in range(NUM_INPUT):\n",
    "            for m in range(batch_size):\n",
    "                grad_W1[i][j] += delta_z1_s[i][m] * X[m][j]\n",
    "                grad_b1[i] += delta_z1_s[i][m]\n",
    "            grad_W1[i][j] /= batch_size\n",
    "            grad_b1[i] /= batch_size\n",
    "    \n",
    "    grad = pack(grad_W1.T, grad_b1, grad_W2.T, grad_b2)\n",
    "  \n",
    "    return grad\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    trainX, trainY = loadData(\"train\")\n",
    "    testX, testY = loadData(\"test\")\n",
    "\n",
    "    print(\"len(trainX): \", len(trainX))\n",
    "    print(\"len(testX): \", len(testX))\n",
    "\n",
    "    # Initialize weights randomly\n",
    "    W1 = 2*(np.random.random(size=(NUM_INPUT, NUM_HIDDEN))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
    "    b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
    "    W2 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_OUTPUT))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "    b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
    "    w = pack(W1, b1.reshape(len(b1),1), W2, b2.reshape(len(b2),1))\n",
    "    \n",
    "    # Check that the gradient is correct on just a few examples (randomly drawn).\n",
    "    idxs = np.random.permutation(trainX.shape[0])[0:NUM_CHECK]\n",
    "    discrepancy = scipy.optimize.check_grad(lambda w_: fCE(np.atleast_2d(trainX[idxs,:]), np.atleast_2d(trainY[idxs,:]), w_, NUM_CHECK)[2], lambda w_: gradCE(np.atleast_2d(trainX[idxs,:]), np.atleast_2d(trainY[idxs,:]), w_, NUM_CHECK), w)\n",
    "    if discrepancy < 0.01:\n",
    "        print(\"My implemented cost and gradient functions are correct\")\n",
    "\n",
    "#     train_acc, test_acc = train(trainX, trainY, testX, testY, w)\n",
    "#     print(train_acc,test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11792891343254716"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_ass08]",
   "language": "python",
   "name": "conda-env-env_ass08-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
