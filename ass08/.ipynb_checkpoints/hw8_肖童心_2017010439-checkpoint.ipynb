{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学号：2017010439\n",
    "# 姓名：肖童心\n",
    "\n",
    "特别声明：本次作业是在李冠达同学的不厌其烦的解答下完成的，在与项雨桐同学的讨论中修正的，在彻底理解了之后自己完成的，没有Ctrl+C，Ctrl+V的任何操作\n",
    "\n",
    "# Deep Learning\n",
    "**Multilayer Perceptron (MLP)**: In this homework you are required to implement and train a 3-layer neural network to classify images of hand-written digits from the MNIST dataset. The input to the network will be a 28 × 28-pixel image, which is converted into a 784-dimensional vector. The output will be a vector of 10 probabilities (one for each digit). Specifically, the network you create should implement a function $g: \\mathbb{R}^{784} \\rightarrow \\mathbb{R}^{10}$, where:\n",
    "\n",
    "$$\\mathbf{z}_{1} = \\mathbf{W}^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)}$$\n",
    "$$\\mathbf{h}_1 = ReLU(\\mathbf{z}_1)$$\n",
    "$$\\mathbf{z}_2 = \\mathbf{W}^{(2)}\\mathbf{h}_1 + \\mathbf{b}^{(2)}$$\n",
    "$$\\hat{\\mathbf{y}} = g(\\mathbf{x}) = Softmax(\\mathbf{z}_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Propagation**: Compute the intermediate outputs $\\mathbf{z}_{1}$, $\\mathbf{h}_{1}$, $\\mathbf{z}_{2}$, and $\\hat{\\mathbf{y}}$ as the directed graph shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./img/mlp.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function**: After forward propagation, you should use the cross-entropy loss function: \n",
    "$$ f_{CE}(\\mathbf{W}^{(1)},\\mathbf{b}^{(1)}, \\mathbf{W}^{(2)}, \\mathbf{b}^{(2)}) =  - \\frac{1}{n}\\sum_{i=1}^{n} \\sum_{k=1}^{10} \\mathbf{y}_k^{(i)} \\log \\hat{\\mathbf{y}}_k^{(i)} $$\n",
    "where $n$ is the number of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backwards Propagation**: To train the neural network, you should use stochastic gradient descent (SGD). \n",
    "\n",
    "# Question 1:\n",
    "Compute the individual gradient for each term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{W}^{(2)}} =   - \\frac{1}{N}\\sum_{n=1}^{N}\\frac{\\partial f_{n}}{\\partial \\mathbf z_2}{({\\mathbf h_1})}^\\mathbf T $$ \n",
    "\n",
    "\n",
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{b}^{(2)}} =     - \\frac{1}{N}\\sum_{n=1}^{N}\\frac{\\partial f_{n}}{\\partial \\mathbf z_2}$$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{W}^{(1)}} =  - \\frac{1}{N}\\sum_{n=1}^{N}\\frac{\\partial f_{n}}{\\partial \\mathbf z_1}{({\\mathbf x})}^\\mathbf T $$\n",
    "    \n",
    "    \n",
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{b}^{(1)}} =     - \\frac{1}{N}\\sum_{n=1}^{N}\\frac{\\partial f_{n}}{\\partial \\mathbf z_1}$$\n",
    "\n",
    "where\n",
    "$$ \\frac{\\partial f_{n}}{\\partial \\mathbf {z}_i^{(2)}}=\\hat{y_i}-y_i$$\n",
    "$$ \\frac{\\partial f_{n}}{\\partial \\mathbf {z}_j^{(1)}}=sgn(\\mathbf{z}_j^{(1)})((\\frac{\\partial f_{n}}{\\partial \\mathbf {z}^{(2)}})^\\mathbf T \\cdot \\mathbf{W}_j^{(2)})^\\mathbf T $$\n",
    "prove\n",
    "$$ \\mathbf{z}_i^{(1)}=    \\mathbf{W}_i^{(1)} \\cdot \\mathbf{x}_i^+\\mathbf{b}_i^{(1)} $$\n",
    "\n",
    "$$ \\mathbf{z}_i^{(2)}=    \\mathbf{W}_i^{(2)} \\cdot \\mathbf{h}_1i^+\\mathbf{b}_i^{(2)} $$\n",
    "\n",
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{b}^{(1)}} =     \\frac{\\partial f_{CE}}{\\partial \\mathbf {z}^{(1)}}$$\n",
    "\n",
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{b}^{(2)}} =     \\frac{\\partial f_{CE}}{\\partial \\mathbf {z}^{(2)}}$$\n",
    "\n",
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{W}^{(1)}} =   \\frac{\\partial f_{CE}}{\\partial \\mathbf {z}^{(1)}}\\cdot \\mathbf{X}^\\mathbf T $$\n",
    "\n",
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{W}^{(2)}} =   \\frac{\\partial f_{CE}}{\\partial \\mathbf {z}^{(2)}}\\cdot \\mathbf{h}_1^\\mathbf T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: \n",
    "Verify that your implemented gradient functions are correct using a numerical derivative approximation in *scipy.optimize.check_grad*\n",
    "\n",
    "See the call to check grad in the starter code. \n",
    "\n",
    "Note that: the discrepancy should be less than 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: \n",
    "Train the network using proper hyper-parameters (batch size, learning rate etc), and report the train accuracy and test accuracy in the *Starter Code* Below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE THAT**: You only need to submit this '.ipynb' file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter Code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize\n",
    "\n",
    "NUM_INPUT = 784  # Number of input neurons\n",
    "NUM_HIDDEN = 50  # Number of hidden neurons\n",
    "NUM_OUTPUT = 10  # Number of output neurons\n",
    "NUM_CHECK = 5  # Number of examples on which to check the gradient\n",
    "\n",
    "# Given a vector w containing all the weights and biased vectors, extract\n",
    "# and return the individual weights and biases W1, b1, W2, b2.\n",
    "# This is useful for performing a gradient check with check_grad.\n",
    "def unpack (w):\n",
    "    W1=w[:39200]\n",
    "    b1=w[39200:39250]\n",
    "    W2=w[39250:39750]\n",
    "    b2=w[39750:]\n",
    "    W1=np.reshape(W1,(50,784))\n",
    "    b1=np.reshape(b1,(50,1))\n",
    "    W2=np.reshape(W2,(10,50))\n",
    "    b2=np.reshape(b2,(10,1))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Given individual weights and biases W1, b1, W2, b2, concatenate them and\n",
    "# return a vector w containing all of them.\n",
    "# This is useful for performing a gradient check with check_grad.\n",
    "def pack (W1, b1, W2, b2):\n",
    "    W1=np.reshape(W1,39200)\n",
    "    b1=np.reshape(b1,50)\n",
    "    W2=np.reshape(W2,500)\n",
    "    b2=np.reshape(b2,10)\n",
    "    w=np.hstack((W1,b1,W2,b2))\n",
    "    return w\n",
    "\n",
    "# Load the images and labels from a specified dataset (train or test).\n",
    "def loadData (which):\n",
    "    images = np.load(\"data/mnist_{}_images.npy\".format(which))\n",
    "    labels = np.load(\"data/mnist_{}_labels.npy\".format(which))\n",
    "    return images, labels\n",
    "\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the cross-entropy (CE) loss. You might\n",
    "# want to extend this function to return multiple arguments (in which case you\n",
    "# will also need to modify slightly the gradient check code below).\n",
    "def ReLU(x):\n",
    "    if x>0:\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def sgn(x):\n",
    "    if x>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def Softmax(z):\n",
    "    size=z.size\n",
    "    z=np.reshape(z,size)\n",
    "    m=np.max(z)\n",
    "    z_m=z-m\n",
    "    for i in range(size):\n",
    "        z_m[i]=np.exp(z_m[i])\n",
    "    y_hat=np.zeros((size,1))\n",
    "    sz_m=np.sum(z_m)\n",
    "    for i in range(size):\n",
    "        y_hat[i][0]=z_m[i]/sz_m\n",
    "    return y_hat\n",
    "\n",
    "def f_n(X, Y, w):\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    X=np.reshape(X,(NUM_INPUT,1))\n",
    "    Y=np.reshape(Y,(NUM_OUTPUT,1))\n",
    "    z_1=np.dot(W1,X)+b1\n",
    "    z_1=np.reshape(z_1,(50,1))\n",
    "    h_1=np.zeros((50,1))\n",
    "    for i in range(50):\n",
    "        h_1[i][0]=ReLU(z_1[i][0])\n",
    "    z_2=np.dot(W2,h_1)+b2\n",
    "    z_2=np.reshape(z_2,(10,1))\n",
    "    y_hat=Softmax(z_2)\n",
    "    log_y_hat=np.zeros((10,1))\n",
    "    for i in range(10):\n",
    "        log_y_hat[i][0]=np.log(y_hat[i][0])\n",
    "    cost_n=-np.dot(Y.T,log_y_hat)\n",
    "    return z_1,h_1,cost_n,y_hat\n",
    "\n",
    "def fCE (X, Y, w):\n",
    "    m=X.shape[:][0]\n",
    "    cost_ni=np.zeros(m)\n",
    "    for i in range(m):\n",
    "        x_i=X[i,:]\n",
    "        y_i=Y[i,:]\n",
    "        z_1, h_1, cost_n, y_hat_i = f_n(x_i, y_i, w)\n",
    "        cost_ni[i]=cost_n\n",
    "    cost=np.mean(cost_ni)\n",
    "    return cost\n",
    "\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the gradient of fCE. You might\n",
    "# want to extend this function to return multiple arguments (in which case you\n",
    "# will also need to modify slightly the gradient check code below).\n",
    "def gradCE_n(X, Y, w):\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    z1, h1, cost_n, y_hat = f_n(X, Y, w)\n",
    "    gr_z2 = y_hat-Y\n",
    "    #gr_W2 = np.dot(gr_z2,h1.T)\n",
    "    \n",
    "    gr_W2 = np.zeros((10,50))\n",
    "    for i in range(10):\n",
    "        gr_W2[i] = np.dot(gr_z2[i,:],h1.T)\n",
    "    \n",
    "    gr_b2 = gr_z2\n",
    "    z1_sgn=np.zeros((50,1))\n",
    "    gr_z1 = np.zeros((50,1))    \n",
    "\n",
    "    for i in range(50):\n",
    "        z1_sgn[i][0]=sgn(z1[i][0])\n",
    "        gr_z1[i][0] = z1_sgn[i][0]* (np.dot(gr_z2.T,W2[:,i]))\n",
    "    #gr_z1 = z1_sgn * np.dot(gr_z2.T,W2).T\n",
    "    gr_W1 = np.dot(gr_z1, X.T)\n",
    "    gr_b1 = gr_z1\n",
    "\n",
    "    grad_n = pack(gr_W1,gr_b1,gr_W2,gr_b2)\n",
    "    \n",
    "    return grad_n\n",
    "\n",
    "def gradCE (X, Y, w):\n",
    "    m=X.shape[0]\n",
    "    grad=np.zeros(39760)\n",
    "    for i in range(m):\n",
    "        x_i=np.reshape(X[i,:],(NUM_INPUT,1))\n",
    "        y_i=np.reshape(Y[i,:],(NUM_OUTPUT,1))\n",
    "        grad_i = gradCE_n(x_i, y_i, w)\n",
    "        grad += grad_i\n",
    "    \n",
    "    grad= grad / m\n",
    "    return grad\n",
    "\n",
    "# Given training and testing datasets and an initial set of weights/biases b,\n",
    "# train the NN.\n",
    "## return the train accuracy and the test accuracy\n",
    "theta = np.zeros(39760)\n",
    "def SGD(X,Y):\n",
    "    m = X.shape[0]\n",
    "    Index = np.random.randint(0, m, 64)#Batch_size\n",
    "    SGD_X = np.zeros((64,784))\n",
    "    SGD_Y = np.zeros((64,10))\n",
    "    for i in range(64):\n",
    "        SGD_X[i,:] = X[Index[i],:]\n",
    "        SGD_Y[i,:] = Y[Index[i],:]\n",
    "    return SGD_X,SGD_Y\n",
    "\n",
    "def onehot(y_hat):\n",
    "    id=0\n",
    "    y_hat = np.reshape(y_hat,10)\n",
    "    maxy = y_hat[0]\n",
    "    for i in range(1,10):\n",
    "        if(y_hat[i]>maxy):\n",
    "            maxy = y_hat[i]\n",
    "            id=i\n",
    "    yflag1=np.zeros(10)\n",
    "    yflag1[id]=1\n",
    "    return yflag1\n",
    "\n",
    "def judge(y_hat):\n",
    "    maxy = np.max(y_hat)\n",
    "    yflag2=np.zeros(10)\n",
    "    for i in range(1,10):\n",
    "        if y_hat[i]-maxy >= -1e-7:\n",
    "            yflag2[i]=1\n",
    "    return yflag2\n",
    "\n",
    "def MLP_acc(X, Y, w):\n",
    "    m=X.shape[0]\n",
    "    acc = 0\n",
    "    for i in range(m):\n",
    "        x_i=X[i,:]\n",
    "        y_i=Y[i,:]\n",
    "        z_1, h_1, cost_n, y_hat_i = f_n(x_i, y_i, w)\n",
    "        pre_y_i1 = onehot(y_hat_i)\n",
    "        pre_y_i2 = judge(y_hat_i)\n",
    "        judgey = pre_y_i1 + pre_y_i2\n",
    "        if max(judgey) == 2:\n",
    "            acc+=1\n",
    "    acc = acc/m\n",
    "    return acc\n",
    "        \n",
    "def totrain(X, Y, w):\n",
    "    MAX = 500 #最大训练次数，500可更改\n",
    "    count = 1\n",
    "    wi=w[:]\n",
    "    step = 0.05 #步长，可以改\n",
    "    cost = np.zeros(MAX)\n",
    "    cost[0]=-999999\n",
    "    toacc=0\n",
    "    print(\"iterator_step\\tleaning_rate\\tcost\\t\\taccuracy\")\n",
    "    while(step > 0.005 and count < MAX-10 and toacc < 0.95):\n",
    "        alpha = min(max(20*step, 0.01), 0.05, abs(cost[count-1])/4)\n",
    "        X, Y = SGD(X, Y)\n",
    "        grad_tr = gradCE(X, Y, wi)\n",
    "        wi -= alpha * grad_tr\n",
    "        cost[count] = fCE(X, Y, wi)\n",
    "        step= abs(cost[count]-cost[count-1])\n",
    "        toacc = MLP_acc(X,Y,wi)\n",
    "        print('%d\\t\\t%.4f\\t\\t%.6f\\t%.4f' % (count, alpha, cost[count], toacc))\n",
    "        count += 1\n",
    "    return wi,toacc\n",
    "    \n",
    "def train (trainX, trainY, testX, testY, w):\n",
    "    w_tr,train_acc=totrain(trainX,trainY,w)\n",
    "    test_acc=MLP_acc(testX,testY,w_tr) \n",
    "    return train_acc, test_acc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    trainX, trainY = loadData(\"train\")\n",
    "    testX, testY = loadData(\"test\")\n",
    "\n",
    "    print(\"len(trainX): \", len(trainX))\n",
    "    print(\"len(testX): \", len(testX))\n",
    "\n",
    "    # Initialize weights randomly\n",
    "    W1 = 2*(np.random.random(size=(NUM_INPUT, NUM_HIDDEN))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
    "    b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
    "    W2 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_OUTPUT))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "    b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
    "    w = pack(W1, b1, W2, b2)\n",
    "    \n",
    "    # Check that the gradient is correct on just a few examples (randomly drawn).\n",
    "    idxs = np.random.permutation(trainX.shape[0])[0:NUM_CHECK]\n",
    "    discrepancy = scipy.optimize.check_grad(lambda w_: fCE(np.atleast_2d(trainX[idxs,:]), np.atleast_2d(trainY[idxs,:]), w_), \\\n",
    "                                    lambda w_: gradCE(np.atleast_2d(trainX[idxs,:]), np.atleast_2d(trainY[idxs,:]), w_), \\\n",
    "                                    w)\n",
    "    if discrepancy < 0.01:\n",
    "        print(\"My implemented cost and gradient functions are correct\")\n",
    "\n",
    "    # Train the network and return the train accuracy and test accuracy\n",
    "#     train_acc, test_acc = train(trainX, trainY, testX, testY, w)\n",
    "#     print(train_acc,test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
